==============================Classic================================

Step 1:
#log_0 train/validation
#log_2 validation

============DT=============
Train validation:
Precision: 0.990, recall: 0.999, F1-measure: 0.994
Test validation:
Precision: 0.917, recall: 0.917, F1-measure: 0.917

============LR=============
Train validation:
Precision: 0.983, recall: 0.999, F1-measure: 0.991
Test validation:
Precision: 0.917, recall: 0.917, F1-measure: 0.917

============SVM=============
Train validation:
Precision: 0.980, recall: 0.999, F1-measure: 0.989
Test validation:
Precision: 0.917, recall: 0.917, F1-measure: 0.917

============MultinomialNB/partial_fit=============
Train validation:
Precision: 0.935, recall: 0.921, F1-measure: 0.928
Test validation:
Precision: 0.909, recall: 0.833, F1-measure: 0.870

============Perceptron/partial_fit=============
Train validation:
Precision: 0.970, recall: 0.857, F1-measure: 0.910
Test validation:
Precision: 1.000, recall: 0.750, F1-measure: 0.857

============SGDC_SVM/partial_fit=============
Train validation:
Precision: 0.925, recall: 0.925, F1-measure: 0.925
Test validation:
Precision: 1.000, recall: 0.917, F1-measure: 0.957

============SGDC_LR/partial_fit=============
Train validation:
Precision: 0.840, recall: 0.933, F1-measure: 0.884
Test validation:
Precision: 0.706, recall: 1.000, F1-measure: 0.828

Step 2:
#log_1 validation

============DT=============
Test validation:
Precision: 0.985, recall: 0.998, F1-measure: 0.991

============LR=============
Test validation:
Precision: 0.974, recall: 0.998, F1-measure: 0.986

============SVM=============
Test validation:
Precision: 0.972, recall: 0.998, F1-measure: 0.985

============MultinomialNB/partial_fit=============
Test validation:
Precision: 0.928, recall: 0.919, F1-measure: 0.923

============Perceptron/partial_fit=============
Test validation:
Precision: 0.972, recall: 0.861, F1-measure: 0.913

============SGDC_SVM/partial_fit=============
Test validation:
Precision: 0.909, recall: 0.922, F1-measure: 0.915

============SGDC_LR/partial_fit=============
Test validation:
Precision: 0.844, recall: 0.924, F1-measure: 0.882

Step 3: partial_fit
#log_2 train/validation
#log_1 validation

============MultinomialNB/partial_fit=============
Train validation:
Precision: 0.917, recall: 0.917, F1-measure: 0.917
Test validation:
Precision: 0.931, recall: 0.944, F1-measure: 0.938

============Perceptron/partial_fit=============
Train validation:
Precision: 0.917, recall: 0.917, F1-measure: 0.917
Test validation:
Precision: 0.974, recall: 0.836, F1-measure: 0.900

============SGDC_SVM/partial_fit=============
Train validation:
Precision: 1.000, recall: 0.917, F1-measure: 0.957
Test validation:
Precision: 0.940, recall: 0.915, F1-measure: 0.927

============SGDC_LR/partial_fit=============
Train validation:
Precision: 0.917, recall: 0.917, F1-measure: 0.917
Test validation:
Precision: 0.918, recall: 0.850, F1-measure: 0.882

Step 4:
#log_3 validation

============DT=============
Test validation:
Precision: 1.000, recall: 0.325, F1-measure: 0.491

============LR=============
Test validation:
Precision: 1.000, recall: 0.297, F1-measure: 0.458

============SVM=============
Test validation:
Precision: 1.000, recall: 0.278, F1-measure: 0.435

============MultinomialNB/partial_fit=============
Test validation:
Precision: 1.000, recall: 0.040, F1-measure: 0.076

============Perceptron/partial_fit=============
Test validation:
Precision: 0.861, recall: 0.012, F1-measure: 0.023

============SGDC_SVM/partial_fit=============
Test validation:
Precision: 0.999, recall: 0.293, F1-measure: 0.454

============SGDC_LR/partial_fit=============
Test validation:
Precision: 1.000, recall: 0.377, F1-measure: 0.547

Step 5: partial_fit
#log_3 train/validation
#log_1 validation

============DT=============
N/A

============LR=============
N/A

============SVM=============
N/A

============MultinomialNB/partial_fit=============
Train validation:
Precision: 0.980, recall: 1.000, F1-measure: 0.990
Test validation:
Precision: 0.850, recall: 0.964, F1-measure: 0.903

============Perceptron/partial_fit=============
Train validation:
Precision: 0.998, recall: 0.999, F1-measure: 0.999
Test validation:
Precision: 0.938, recall: 0.874, F1-measure: 0.905

============SGDC_SVM/partial_fit=============
Train validation:
Precision: 0.993, recall: 1.000, F1-measure: 0.996
Test validation:
Precision: 0.898, recall: 0.892, F1-measure: 0.895

============SGDC_LR/partial_fit=============
Train validation:
Precision: 0.999, recall: 0.990, F1-measure: 0.995
Test validation:
Precision: 0.897, recall: 0.933, F1-measure: 0.914

-------------------------top level scripts--------------------------------------

# han @ coffee in ~/Workspace/loganalyzer/entrance on git:develop x [22:29:15]
$ ./top_static_unix.sh
Pre-processing the raw train dataset ...
Purge costs 0:00:02.648125

Parsing file: /Users/han/Workspace/loganalyzer/logs/train_norm.txt
Progress: |||||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% Complete
Parsing done. [Time taken: 0:00:14.780718]

Pre-processing the raw test dataset ...
Purge costs 0:00:00.096589

Parsing file: /Users/han/Workspace/loganalyzer/logs/test_norm.txt
Progress: |||||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% Complete
Parsing done. [Time taken: 0:00:00.606931]

===> Train Module: DecesionTree

The number of anomaly logs is 6663, but it requires further processing
There are 1217 instances (sliding windows) in this dataset, cost 0:00:00.062251

There are 395 log events
Among all instances, 683 are anomalies
====== Transformed train data summary ======
Final train data shape: 1217-by-395

Normal training...: DecesionTree

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list in templates: static version.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.001917

There are 250 log events
Among all instances, 12 are anomalies
====== Transformed test data summary ======
Test data shape: 27-by-395

Train validation:
Precision: 0.990, recall: 0.999, F1-measure: 0.994

Test validation:
Precision: 0.917, recall: 0.917, F1-measure: 0.917

===> Train Module: LR

The number of anomaly logs is 6663, but it requires further processing
Loading shuffled EventId list in templates: static version.
There are 1217 instances (sliding windows) in this dataset, cost 0:00:00.063191

There are 395 log events
Among all instances, 683 are anomalies
====== Transformed train data summary ======
Final train data shape: 1217-by-395

Normal training...: LR

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list in templates: static version.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.001990

There are 250 log events
Among all instances, 12 are anomalies
====== Transformed test data summary ======
Test data shape: 27-by-395

Train validation:
Precision: 0.983, recall: 0.999, F1-measure: 0.991

Test validation:
Precision: 0.917, recall: 0.917, F1-measure: 0.917

===> Train Module: SVM

The number of anomaly logs is 6663, but it requires further processing
Loading shuffled EventId list in templates: static version.
There are 1217 instances (sliding windows) in this dataset, cost 0:00:00.063339

There are 395 log events
Among all instances, 683 are anomalies
====== Transformed train data summary ======
Final train data shape: 1217-by-395

Normal training...: SVM

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list in templates: static version.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.002045

There are 250 log events
Among all instances, 12 are anomalies
====== Transformed test data summary ======
Test data shape: 27-by-395

Train validation:
Precision: 0.981, recall: 0.999, F1-measure: 0.990

Test validation:
Precision: 0.917, recall: 0.917, F1-measure: 0.917

Pre-processing the raw test dataset ...
Purge costs 0:00:01.539346

Parsing file: /Users/han/Workspace/loganalyzer/logs/test_norm.txt
Progress: |||||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% Complete
Parsing done. [Time taken: 0:00:08.384348]

===> Predict Model: DecesionTree.onnx

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: static version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.035229

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-395

Test validation:
Precision: 0.985, recall: 0.998, F1-measure: 0.991

===> Predict Model: LR.onnx

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: static version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.036687

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-395

Test validation:
Precision: 0.974, recall: 0.998, F1-measure: 0.986

===> Predict Model: SVM.onnx

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: static version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.035133

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-395

Test validation:
Precision: 0.972, recall: 0.998, F1-measure: 0.985


# han @ coffee in ~/Workspace/loganalyzer/entrance on git:develop x [22:30:06]
$ ./top_pfit_unix.sh
Pre-processing the raw train dataset ...
Purge costs 0:00:02.647358

Parsing file: /Users/han/Workspace/loganalyzer/logs/train_norm.txt
Progress: |||||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% Complete
Parsing done. [Time taken: 0:00:14.821904]

Pre-processing the raw test dataset ...
Purge costs 0:00:00.096591

Parsing file: /Users/han/Workspace/loganalyzer/logs/test_norm.txt
Progress: |||||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% Complete
Parsing done. [Time taken: 0:00:00.587581]

===> Train Module: MultinomialNB

The number of anomaly logs is 6663, but it requires further processing
There are 1217 instances (sliding windows) in this dataset, cost 0:00:00.062033

There are 395 log events
Among all instances, 683 are anomalies
====== Transformed train data summary ======
Final train data shape: 1217-by-2000

First time training...: MultinomialNB

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.001995

There are 250 log events
Among all instances, 12 are anomalies
====== Transformed test data summary ======
Test data shape: 27-by-2000

Train validation:
Precision: 0.935, recall: 0.921, F1-measure: 0.928

Test validation:
Precision: 0.909, recall: 0.833, F1-measure: 0.870

===> Train Module: Perceptron

The number of anomaly logs is 6663, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 1217 instances (sliding windows) in this dataset, cost 0:00:00.059532

There are 395 log events
Among all instances, 683 are anomalies
====== Transformed train data summary ======
Final train data shape: 1217-by-2000

First time training...: Perceptron

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.003329

There are 250 log events
Among all instances, 12 are anomalies
====== Transformed test data summary ======
Test data shape: 27-by-2000

Train validation:
Precision: 0.868, recall: 0.892, F1-measure: 0.879

Test validation:
Precision: 0.733, recall: 0.917, F1-measure: 0.815

===> Train Module: SGDC_SVM

The number of anomaly logs is 6663, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 1217 instances (sliding windows) in this dataset, cost 0:00:00.059663

There are 395 log events
Among all instances, 683 are anomalies
====== Transformed train data summary ======
Final train data shape: 1217-by-2000

First time training...: SGDC_SVM

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.003378

There are 250 log events
Among all instances, 12 are anomalies
====== Transformed test data summary ======
Test data shape: 27-by-2000

Train validation:
Precision: 0.919, recall: 0.893, F1-measure: 0.906

Test validation:
Precision: 0.846, recall: 0.917, F1-measure: 0.880

===> Train Module: SGDC_LR

The number of anomaly logs is 6663, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 1217 instances (sliding windows) in this dataset, cost 0:00:00.061628

There are 395 log events
Among all instances, 683 are anomalies
====== Transformed train data summary ======
Final train data shape: 1217-by-2000

First time training...: SGDC_LR

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.001995

There are 250 log events
Among all instances, 12 are anomalies
====== Transformed test data summary ======
Test data shape: 27-by-2000

Train validation:
Precision: 0.891, recall: 0.934, F1-measure: 0.912

Test validation:
Precision: 0.846, recall: 0.917, F1-measure: 0.880

Pre-processing the raw test dataset ...
Purge costs 0:00:01.571230

Parsing file: /Users/han/Workspace/loganalyzer/logs/test_norm.txt
Progress: |||||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% Complete
Parsing done. [Time taken: 0:00:08.296831]

===> Predict Model: MultinomialNB.onnx

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.034510

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-2000

Test validation:
Precision: 0.928, recall: 0.919, F1-measure: 0.923

===> Predict Model: Perceptron.onnx

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.036100

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-2000

Test validation:
Precision: 0.885, recall: 0.877, F1-measure: 0.881

===> Predict Model: SGDC_SVM.onnx

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.034706

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-2000

Test validation:
Precision: 0.926, recall: 0.892, F1-measure: 0.909

===> Predict Model: SGDC_LR.onnx

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.034960

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-2000

Test validation:
Precision: 0.896, recall: 0.946, F1-measure: 0.920

Pre-processing the raw train dataset ...
Purge costs 0:00:00.098784

Parsing file: /Users/han/Workspace/loganalyzer/logs/train_norm.txt
Progress: |||||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% Complete
Parsing done. [Time taken: 0:00:00.590300]

Pre-processing the raw test dataset ...
Purge costs 0:00:01.526631

Parsing file: /Users/han/Workspace/loganalyzer/logs/test_norm.txt
Progress: |||||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% Complete
Parsing done. [Time taken: 0:00:08.241576]

===> Train Module: MultinomialNB

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
11 new template IDs are inserted to STIDLE.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.002303

There are 250 log events
Among all instances, 12 are anomalies
====== Transformed train data summary ======
Final train data shape: 27-by-2000

Incremental training...: MultinomialNB

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.033679

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-2000

Train validation:
Precision: 0.917, recall: 0.917, F1-measure: 0.917

Test validation:
Precision: 0.931, recall: 0.944, F1-measure: 0.938

===> Train Module: Perceptron

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
0 new template IDs are inserted to STIDLE.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.002096

There are 250 log events
Among all instances, 12 are anomalies
====== Transformed train data summary ======
Final train data shape: 27-by-2000

Incremental training...: Perceptron

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.034450

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-2000

Train validation:
Precision: 0.750, recall: 1.000, F1-measure: 0.857

Test validation:
Precision: 0.853, recall: 0.857, F1-measure: 0.855

===> Train Module: SGDC_SVM

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
0 new template IDs are inserted to STIDLE.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.002029

There are 250 log events
Among all instances, 12 are anomalies
====== Transformed train data summary ======
Final train data shape: 27-by-2000

Incremental training...: SGDC_SVM

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.034057

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-2000

Train validation:
Precision: 1.000, recall: 0.917, F1-measure: 0.957

Test validation:
Precision: 0.954, recall: 0.892, F1-measure: 0.922

===> Train Module: SGDC_LR

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
0 new template IDs are inserted to STIDLE.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.002275

There are 250 log events
Among all instances, 12 are anomalies
====== Transformed train data summary ======
Final train data shape: 27-by-2000

Incremental training...: SGDC_LR

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.033586

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-2000

Train validation:
Precision: 0.857, recall: 1.000, F1-measure: 0.923

Test validation:
Precision: 0.900, recall: 0.930, F1-measure: 0.915

Pre-processing the raw test dataset ...
Purge costs 0:00:18.436058

Parsing file: /Users/han/Workspace/loganalyzer/logs/test_norm.txt
Progress: |||||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% Complete
Parsing done. [Time taken: 0:01:34.881696]

===> Predict Model: MultinomialNB.onnx

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.364661

There are 176 log events
Among all instances, 2658 are anomalies
====== Transformed test data summary ======
Test data shape: 5664-by-2000

Test validation:
Precision: 1.000, recall: 0.040, F1-measure: 0.076

===> Predict Model: Perceptron.onnx

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.361765

There are 176 log events
Among all instances, 2658 are anomalies
====== Transformed test data summary ======
Test data shape: 5664-by-2000

Test validation:
Precision: 0.973, recall: 0.428, F1-measure: 0.594

===> Predict Model: SGDC_SVM.onnx

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.359137

There are 176 log events
Among all instances, 2658 are anomalies
====== Transformed test data summary ======
Test data shape: 5664-by-2000

Test validation:
Precision: 0.999, recall: 0.395, F1-measure: 0.566

===> Predict Model: SGDC_LR.onnx

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.366033

There are 176 log events
Among all instances, 2658 are anomalies
====== Transformed test data summary ======
Test data shape: 5664-by-2000

Test validation:
Precision: 0.986, recall: 0.325, F1-measure: 0.489

Pre-processing the raw train dataset ...
Purge costs 0:00:18.389085

Parsing file: /Users/han/Workspace/loganalyzer/logs/train_norm.txt
Progress: |||||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% Complete
Parsing done. [Time taken: 0:01:35.209288]

Pre-processing the raw test dataset ...
Purge costs 0:00:01.536624

Parsing file: /Users/han/Workspace/loganalyzer/logs/test_norm.txt
Progress: |||||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% Complete
Parsing done. [Time taken: 0:00:08.342947]

===> Train Module: MultinomialNB

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
12 new template IDs are inserted to STIDLE.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.368600

There are 176 log events
Among all instances, 2658 are anomalies
====== Transformed train data summary ======
Final train data shape: 5664-by-2000

Incremental training...: MultinomialNB

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.033739

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-2000

Train validation:
Precision: 0.980, recall: 1.000, F1-measure: 0.990

Test validation:
Precision: 0.850, recall: 0.964, F1-measure: 0.903

===> Train Module: Perceptron

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
0 new template IDs are inserted to STIDLE.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.371825

There are 176 log events
Among all instances, 2658 are anomalies
====== Transformed train data summary ======
Final train data shape: 5664-by-2000

Incremental training...: Perceptron

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.033068

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-2000

Train validation:
Precision: 0.998, recall: 1.000, F1-measure: 0.999

Test validation:
Precision: 0.825, recall: 0.953, F1-measure: 0.884

===> Train Module: SGDC_SVM

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
0 new template IDs are inserted to STIDLE.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.374990

There are 176 log events
Among all instances, 2658 are anomalies
====== Transformed train data summary ======
Final train data shape: 5664-by-2000

Incremental training...: SGDC_SVM

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.034084

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-2000

Train validation:
Precision: 1.000, recall: 0.980, F1-measure: 0.990

Test validation:
Precision: 0.928, recall: 0.899, F1-measure: 0.913

===> Train Module: SGDC_LR

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
0 new template IDs are inserted to STIDLE.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.381368

There are 176 log events
Among all instances, 2658 are anomalies
====== Transformed train data summary ======
Final train data shape: 5664-by-2000

Incremental training...: SGDC_LR

The number of anomaly logs is 2735, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 759 instances (sliding windows) in this dataset, cost 0:00:00.033587

There are 353 log events
Among all instances, 446 are anomalies
====== Transformed test data summary ======
Test data shape: 759-by-2000

Train validation:
Precision: 0.999, recall: 0.998, F1-measure: 0.999

Test validation:
Precision: 0.851, recall: 0.957, F1-measure: 0.901

Pre-processing the raw test dataset ...
Purge costs 0:00:18.866407

Parsing file: /Users/han/Workspace/loganalyzer/logs/test_norm.txt
Progress: |||||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% Complete
Warning: template is duplicated, merging.
Parsing done. [Time taken: 0:01:35.898440]

===> Predict Model: MultinomialNB.onnx

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.403029

There are 176 log events
Among all instances, 2658 are anomalies
====== Transformed test data summary ======
Test data shape: 5664-by-2000

Test validation:
Precision: 0.980, recall: 1.000, F1-measure: 0.990

===> Predict Model: Perceptron.onnx

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.393172

There are 176 log events
Among all instances, 2658 are anomalies
====== Transformed test data summary ======
Test data shape: 5664-by-2000

Test validation:
Precision: 0.998, recall: 1.000, F1-measure: 0.999

===> Predict Model: SGDC_SVM.onnx

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.366510

There are 176 log events
Among all instances, 2658 are anomalies
====== Transformed test data summary ======
Test data shape: 5664-by-2000

Test validation:
Precision: 1.000, recall: 0.980, F1-measure: 0.990

===> Predict Model: SGDC_LR.onnx

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list in templates: incremental update version.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.363635

There are 176 log events
Among all instances, 2658 are anomalies
====== Transformed test data summary ======
Test data shape: 5664-by-2000

Test validation:
Precision: 0.999, recall: 0.998, F1-measure: 0.999


==============================DeepLog================================

---------epoch=1, hidden=64, win=10-----------
===> Start training the execution path model ...
Slicing the multi-session logs with window 10 ...
Epoch 1/1, train loss: 2.34789
Train Dataset Validation ==> TP: 0, FP: 7827, TN: 196515, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 503, FP: 372, TN: 803, FN: 4
Test Dataset Validation  ==> Precision: 57.49%, Recall: 99.21%, F1: 72.79%

---------epoch=1, hidden=128, win=10-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/1, train loss: 1.42619
Train Dataset Validation ==> TP: 0, FP: 2034, TN: 202308, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 503, FP: 313, TN: 862, FN: 4
Test Dataset Validation  ==> Precision: 61.64%, Recall: 99.21%, F1: 76.04%

---------epoch=1, hidden=256, win=10-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/1, train loss: 0.86946
Train Dataset Validation ==> TP: 0, FP: 973, TN: 203369, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 500, FP: 320, TN: 855, FN: 7
Test Dataset Validation  ==> Precision: 60.98%, Recall: 98.62%, F1: 75.36%

---------epoch=1, hidden=512, win=10-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/1, train loss: 0.67083
Train Dataset Validation ==> TP: 0, FP: 538, TN: 203804, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 500, FP: 299, TN: 876, FN: 7
Test Dataset Validation  ==> Precision: 62.58%, Recall: 98.62%, F1: 76.57%

---------epoch=10, hidden=256, win=10-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/10, train loss: 0.86775
Epoch 2/10, train loss: 0.28339
Epoch 3/10, train loss: 0.24785
Epoch 4/10, train loss: 0.23422
Epoch 5/10, train loss: 0.22111
Epoch 6/10, train loss: 0.21773
Epoch 7/10, train loss: 0.21918
Epoch 8/10, train loss: 0.21071
Epoch 9/10, train loss: 0.21315
Epoch 10/10, train loss: 0.20986
Train Dataset Validation ==> TP: 0, FP: 62, TN: 204280, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 501, FP: 299, TN: 876, FN: 6
Test Dataset Validation  ==> Precision: 62.62%, Recall: 98.82%, F1: 76.66%

---------epoch=40, hidden=512, win=10-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/40, train loss: 0.63961
Epoch 2/40, train loss: 0.27082
Epoch 3/40, train loss: 0.25060
Epoch 29/40, train loss: 0.18509
Epoch 30/40, train loss: 0.18986
Epoch 31/40, train loss: 0.18726
Epoch 32/40, train loss: 0.18230
Epoch 33/40, train loss: 0.18435
Epoch 34/40, train loss: 0.18332
Epoch 35/40, train loss: 0.18214
Epoch 36/40, train loss: 0.18348
Epoch 37/40, train loss: 0.18778
Epoch 38/40, train loss: 0.18637
Epoch 39/40, train loss: 0.17900
Epoch 40/40, train loss: 0.18091
Train Dataset Validation ==> TP: 0, FP: 63, TN: 204279, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 609, FP: 182, TN: 840, FN: 51
Test Dataset Validation  ==> Precision: 76.99%, Recall: 92.27%, F1: 83.94%


---------nolog4, epoch=10, hidden=64, win=10-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/10, train loss: 3.98030
Epoch 2/10, train loss: 1.92238
Epoch 3/10, train loss: 1.21155
Epoch 4/10, train loss: 0.88148
Epoch 5/10, train loss: 0.71415
Epoch 6/10, train loss: 0.60162
Epoch 7/10, train loss: 0.52559
Epoch 8/10, train loss: 0.48058
Epoch 9/10, train loss: 0.43728
Epoch 10/10, train loss: 0.40217
Train Dataset Validation ==> TP: 0, FP: 800, TN: 71669, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 611, FP: 335, TN: 687, FN: 49
Test Dataset Validation  ==> Precision: 64.59%, Recall: 92.58%, F1: 76.09%

---------nolog4, epoch=10, hidden=64, win=15-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Epoch 1/10, train loss: 3.71009
Epoch 2/10, train loss: 1.55895
Epoch 3/10, train loss: 0.96212
Epoch 4/10, train loss: 0.69904
Epoch 5/10, train loss: 0.55078
Epoch 6/10, train loss: 0.46317
Epoch 7/10, train loss: 0.40202
Epoch 8/10, train loss: 0.35984
Epoch 9/10, train loss: 0.32663
Epoch 10/10, train loss: 0.30823
Train Dataset Validation ==> TP: 0, FP: 414, TN: 71105, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Test Dataset Validation  ==> TP: 623, FP: 308, TN: 669, FN: 72
Test Dataset Validation  ==> Precision: 66.92%, Recall: 89.64%, F1: 76.63%

---------nolog4, epoch=20, hidden=64, win=15-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Epoch 1/20, train loss: 3.90634
Epoch 2/20, train loss: 1.69969
Epoch 3/20, train loss: 1.01466
Epoch 4/20, train loss: 0.72672
Epoch 5/20, train loss: 0.57623
Epoch 6/20, train loss: 0.47708
Epoch 7/20, train loss: 0.41667
Epoch 8/20, train loss: 0.36710
Epoch 9/20, train loss: 0.33029
Epoch 10/20, train loss: 0.31133
Epoch 11/20, train loss: 0.28890
Epoch 12/20, train loss: 0.26585
Epoch 13/20, train loss: 0.25450
Epoch 14/20, train loss: 0.24009
Epoch 15/20, train loss: 0.23171
Epoch 16/20, train loss: 0.22091
Epoch 17/20, train loss: 0.21519
Epoch 18/20, train loss: 0.20537
Epoch 19/20, train loss: 0.20059
Epoch 20/20, train loss: 0.19415
Train Dataset Validation ==> TP: 0, FP: 119, TN: 71400, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Test Dataset Validation  ==> TP: 635, FP: 308, TN: 669, FN: 60
Test Dataset Validation  ==> Precision: 67.34%, Recall: 91.37%, F1: 77.53%

---------nolog4, epoch=50, hidden=64, win=15-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Epoch 1/50, train loss: 3.93942
Epoch 2/50, train loss: 1.77968
Epoch 3/50, train loss: 1.03055
Epoch 4/50, train loss: 0.71584
Epoch 5/50, train loss: 0.56388
Epoch 38/50, train loss: 0.15876
Epoch 39/50, train loss: 0.15280
Epoch 40/50, train loss: 0.14995
Epoch 41/50, train loss: 0.14801
Epoch 42/50, train loss: 0.14627
Epoch 43/50, train loss: 0.15336
Epoch 44/50, train loss: 0.15588
Epoch 45/50, train loss: 0.13912
Epoch 46/50, train loss: 0.14865
Epoch 47/50, train loss: 0.13938
Epoch 48/50, train loss: 0.14578
Epoch 49/50, train loss: 0.13888
Epoch 50/50, train loss: 0.14092
Train Dataset Validation ==> TP: 0, FP: 24, TN: 71495, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Test Dataset Validation  ==> TP: 637, FP: 295, TN: 682, FN: 58
Test Dataset Validation  ==> Precision: 68.35%, Recall: 91.65%, F1: 78.30%

---------nolog4, epoch=50, hidden=128, win=15-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Epoch 1/50, train loss: 2.62818
Epoch 2/50, train loss: 0.78153
Epoch 3/50, train loss: 0.45300
Epoch 4/50, train loss: 0.34277
Epoch 5/50, train loss: 0.27756
Epoch 39/50, train loss: 0.12820
Epoch 40/50, train loss: 0.13650
Epoch 41/50, train loss: 0.12502
Epoch 42/50, train loss: 0.15049
Epoch 43/50, train loss: 0.13674
Epoch 44/50, train loss: 0.13264
Epoch 45/50, train loss: 0.13832
Epoch 46/50, train loss: 0.13198
Epoch 47/50, train loss: 0.13285
Epoch 48/50, train loss: 0.14064
Epoch 49/50, train loss: 0.13132
Epoch 50/50, train loss: 0.13795
Train Dataset Validation ==> TP: 0, FP: 10, TN: 71509, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Test Dataset Validation  ==> TP: 648, FP: 285, TN: 692, FN: 47
Test Dataset Validation  ==> Precision: 69.45%, Recall: 93.24%, F1: 79.61%

---------epoch=300, hidden=128, win=15-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Epoch 1/300, train loss: 2.75612
Epoch 2/300, train loss: 0.86305
Epoch 3/300, train loss: 0.50623
Epoch 4/300, train loss: 0.37446
Epoch 45/300, train loss: 0.13068
Epoch 46/300, train loss: 0.13196
Epoch 47/300, train loss: 0.13028
Epoch 48/300, train loss: 0.13536
Epoch 49/300, train loss: 0.12556
Epoch 50/300, train loss: 0.12838
Epoch 51/300, train loss: 0.12882
Epoch 52/300, train loss: 0.13361
Epoch 53/300, train loss: 0.12697
Epoch 95/300, train loss: 0.10975
Epoch 96/300, train loss: 0.11831
Epoch 97/300, train loss: 0.11231
Epoch 98/300, train loss: 0.14519
Epoch 99/300, train loss: 0.11714
Epoch 100/300, train loss: 0.11384
Epoch 101/300, train loss: 0.11622
Epoch 130/300, train loss: 0.11297
Epoch 131/300, train loss: 0.11060
Epoch 160/300, train loss: 0.11334
Epoch 161/300, train loss: 0.11697
Epoch 162/300, train loss: 0.10709
Epoch 163/300, train loss: 0.11530
Epoch 193/300, train loss: 0.13242
Epoch 194/300, train loss: 0.11132
Epoch 195/300, train loss: 0.11000
Epoch 196/300, train loss: 0.11290
Epoch 197/300, train loss: 0.11277
Epoch 198/300, train loss: 0.11563
Epoch 199/300, train loss: 0.11771
Epoch 200/300, train loss: 0.12417
Epoch 201/300, train loss: 0.12453
Epoch 264/300, train loss: 0.12026
Epoch 265/300, train loss: 0.11622
Epoch 266/300, train loss: 0.12380
Epoch 267/300, train loss: 0.12722
Epoch 299/300, train loss: 0.14052
Epoch 300/300, train loss: 0.13801
Train Dataset Validation ==> TP: 0, FP: 58, TN: 72653, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Test Dataset Validation  ==> TP: 645, FP: 182, TN: 795, FN: 50
Test Dataset Validation  ==> Precision: 77.99%, Recall: 92.81%, F1: 84.76%


---------epoch=50, hidden=128, win=15, normal_0/1/2-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Epoch 1/50, train loss: 5.07849
Epoch 2/50, train loss: 4.08859
Epoch 3/50, train loss: 3.58933
Epoch 4/50, train loss: 3.18508
Epoch 49/50, train loss: 0.30498
Epoch 50/50, train loss: 0.26244
Train Dataset Validation ==> TP: 0, FP: 0, TN: 5197, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Test Dataset Validation  ==> TP: 618, FP: 184, TN: 793, FN: 77
Test Dataset Validation  ==> Precision: 77.06%, Recall: 88.92%, F1: 82.57%

---------epoch=50, hidden=64, win=15, normal_0/1/2-----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Epoch 1/50, train loss: 5.33145
Epoch 2/50, train loss: 4.57109
Epoch 49/50, train loss: 0.57072
Epoch 50/50, train loss: 0.68493
Train Dataset Validation ==> TP: 0, FP: 16, TN: 5181, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Test Dataset Validation  ==> TP: 625, FP: 205, TN: 772, FN: 70
Test Dataset Validation  ==> Precision: 75.30%, Recall: 89.93%, F1: 81.97%

--------epoch=100, hidden=128, win=15------------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Epoch 1/100, train loss: 2.71784
Epoch 2/100, train loss: 0.86354
Epoch 50/100, train loss: 0.12852
Epoch 51/100, train loss: 0.13469
Epoch 95/100, train loss: 0.12361
Epoch 96/100, train loss: 0.12299
Epoch 97/100, train loss: 0.12298
Epoch 98/100, train loss: 0.12057
Epoch 99/100, train loss: 0.12173
Epoch 100/100, train loss: 0.11765
Train Dataset Validation ==> TP: 0, FP: 1, TN: 72710, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Test Dataset Validation  ==> TP: 640, FP: 179, TN: 798, FN: 55
Test Dataset Validation  ==> Precision: 78.14%, Recall: 92.09%, F1: 84.54%

--------epoch=100, hidden=128, win=10----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/100, train loss: 2.84791
Epoch 2/100, train loss: 0.88179
Epoch 50/100, train loss: 0.15823
Epoch 51/100, train loss: 0.16300
Epoch 99/100, train loss: 0.15520
Epoch 100/100, train loss: 0.15158
Train Dataset Validation ==> TP: 0, FP: 40, TN: 73636, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 611, FP: 211, TN: 811, FN: 49
Test Dataset Validation  ==> Precision: 74.33%, Recall: 92.58%, F1: 82.46%

--------epoch=200, hidden=128, win=10----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/200, train loss: 2.81116
Epoch 2/200, train loss: 0.91773
Epoch 50/200, train loss: 0.16097
Epoch 51/200, train loss: 0.16225
Epoch 100/200, train loss: 0.15224
Epoch 199/200, train loss: 0.16769
Epoch 200/200, train loss: 0.15647
Train Dataset Validation ==> TP: 0, FP: 14, TN: 73662, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 616, FP: 196, TN: 826, FN: 44
Test Dataset Validation  ==> Precision: 75.86%, Recall: 93.33%, F1: 83.70%
--------epoch=150, hidden=128, win=15----------
Epoch 100/100, train loss: 0.11765
Train Dataset Validation ==> TP: 0, FP: 0, TN: 72710, FN: 0


--------epoch=300, hidden=128, win=10----------

===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/300, train loss: 2.76781
Epoch 2/300, train loss: 0.84473
Epoch 100/300, train loss: 0.14799
Epoch 101/300, train loss: 0.14049
Epoch 102/300, train loss: 0.14676
Epoch 199/300, train loss: 0.14147
Epoch 200/300, train loss: 0.14472
Epoch 201/300, train loss: 0.16890
Epoch 202/300, train loss: 0.16090
Epoch 298/300, train loss: 0.14741
Epoch 299/300, train loss: 0.14518
Epoch 300/300, train loss: 0.14628
Train Dataset Validation ==> TP: 0, FP: 9, TN: 73667, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 611, FP: 214, TN: 808, FN: 49
Test Dataset Validation  ==> Precision: 74.06%, Recall: 92.58%, F1: 82.29%

--------epoch=150, hidden=128, win=15, new session----------
===> Start training the execution path model ...
Slicing the multi-session logs with window 15 ...
Epoch 1/150, train loss: 2.58557
Epoch 2/150, train loss: 0.63773
Epoch 3/150, train loss: 0.37888
Epoch 4/150, train loss: 0.29071
Epoch 5/150, train loss: 0.24456
Epoch 100/150, train loss: 0.12241
Epoch 101/150, train loss: 0.10721
Epoch 148/150, train loss: 0.10972
Epoch 149/150, train loss: 0.11902
Epoch 150/150, train loss: 0.11131
Train Dataset Validation ==> TP: 0, FP: 1, TN: 77930, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Test Dataset Validation  ==> TP: 630, FP: 218, TN: 759, FN: 65
Test Dataset Validation  ==> Precision: 74.29%, Recall: 90.65%, F1: 81.66%

--------epoch=350, hidden=128, win=10, new session----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/350, train loss: 2.56332
Epoch 2/350, train loss: 0.64688
Epoch 3/350, train loss: 0.40092
Epoch 4/350, train loss: 0.32087
Epoch 5/350, train loss: 0.27886
Epoch 6/350, train loss: 0.24442
Epoch 7/350, train loss: 0.22498
Epoch 8/350, train loss: 0.21108
Epoch 9/350, train loss: 0.19863
Epoch 10/350, train loss: 0.19258
Epoch 19/350, train loss: 0.16596
Epoch 20/350, train loss: 0.16600
Epoch 21/350, train loss: 0.16197
Epoch 22/350, train loss: 0.15803
Epoch 23/350, train loss: 0.15538
Epoch 100/350, train loss: 0.14682
Epoch 101/350, train loss: 0.13525
Epoch 201/350, train loss: 0.14616
Epoch 202/350, train loss: 0.13228
Epoch 203/350, train loss: 0.14033
Epoch 300/350, train loss: 0.15511
Epoch 301/350, train loss: 0.13880
Epoch 346/350, train loss: 0.15690
Epoch 347/350, train loss: 0.15896
Epoch 348/350, train loss: 0.17368
Epoch 349/350, train loss: 0.15398
Epoch 350/350, train loss: 0.16434
Train Dataset Validation ==> TP: 0, FP: 1, TN: 78895, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 604, FP: 211, TN: 811, FN: 56
Test Dataset Validation  ==> Precision: 74.11%, Recall: 91.52%, F1: 81.90%

--------2020/9/3
--------epoch=150, hidden=128, win=15----------

===> Start training the execution path model ...
Slicing the multi-session logs with window 15 ...
Epoch 1/150, train loss: 2.64648
Epoch 2/150, train loss: 0.66544
Epoch 3/150, train loss: 0.38238
Epoch 4/150, train loss: 0.28404
Epoch 5/150, train loss: 0.23693
Epoch 98/150, train loss: 0.11893
Epoch 99/150, train loss: 0.11366
Epoch 100/150, train loss: 0.12355
Epoch 101/150, train loss: 0.11514
Epoch 102/150, train loss: 0.11855
Epoch 126/150, train loss: 0.11021
Epoch 127/150, train loss: 0.11137
Epoch 149/150, train loss: 0.12216
Epoch 150/150, train loss: 0.12038
Train Dataset Validation ==> TP: 0, FP: 2, TN: 78297, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Test Dataset Validation  ==> TP: 642, FP: 202, TN: 792, FN: 62
Test Dataset Validation  ==> Precision: 76.07%, Recall: 91.19%, F1: 82.95%

--------epoch=350, hidden=128, win=10----------

===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/350, train loss: 2.81230
Epoch 2/350, train loss: 0.82248
Epoch 3/350, train loss: 0.47010
Epoch 100/350, train loss: 0.12072
Epoch 101/350, train loss: 0.11598
Epoch 102/350, train loss: 0.11870
Epoch 148/350, train loss: 0.11186
Epoch 149/350, train loss: 0.11670
Epoch 150/350, train loss: 0.11593
Epoch 151/350, train loss: 0.12074
Epoch 152/350, train loss: 0.11268
Epoch 200/350, train loss: 0.11766
Epoch 201/350, train loss: 0.14638
Epoch 202/350, train loss: 0.13828
Epoch 248/350, train loss: 0.12794
Epoch 249/350, train loss: 0.11940
Epoch 250/350, train loss: 0.12179
Epoch 251/350, train loss: 0.11589
Epoch 252/350, train loss: 0.11864
Epoch 300/350, train loss: 0.11369
Epoch 301/350, train loss: 0.11777
Epoch 302/350, train loss: 0.11538
Epoch 349/350, train loss: 0.13157
Epoch 350/350, train loss: 0.12317
Train Dataset Validation ==> TP: 0, FP: 8, TN: 79256, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 588, FP: 209, TN: 830, FN: 81
Test Dataset Validation  ==> Precision: 73.78%, Recall: 87.89%, F1: 80.22%

--------epoch=150, hidden=128, win=10----------

===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/150, train loss: 2.88159
Epoch 2/150, train loss: 0.88130
Epoch 3/150, train loss: 0.47410
Epoch 99/150, train loss: 0.12065
Epoch 100/150, train loss: 0.12359
Epoch 101/150, train loss: 0.12184
Epoch 102/150, train loss: 0.12357
Epoch 103/150, train loss: 0.12161
Epoch 148/150, train loss: 0.12027
Epoch 149/150, train loss: 0.11803
Epoch 150/150, train loss: 0.11862
Train Dataset Validation ==> TP: 0, FP: 4, TN: 79260, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 600, FP: 234, TN: 805, FN: 69
Test Dataset Validation  ==> Precision: 71.94%, Recall: 89.69%, F1: 79.84%

--------epoch=150, hidden=128, win=15, new templates----------
===> Start training the execution path model ...
Slicing the multi-session logs with window 15 ...
Epoch 1/150, train loss: 2.54350
Epoch 2/150, train loss: 0.59760
Epoch 3/150, train loss: 0.36080
Epoch 99/150, train loss: 0.11755
Epoch 100/150, train loss: 0.11758
Epoch 101/150, train loss: 0.11635
Epoch 149/150, train loss: 0.11999
Epoch 150/150, train loss: 0.11423
Train Dataset Validation ==> TP: 0, FP: 6, TN: 78293, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Test Dataset Validation  ==> TP: 643, FP: 206, TN: 788, FN: 61
Test Dataset Validation  ==> Precision: 75.74%, Recall: 91.34%, F1: 82.81%

--------epoch=150, hidden=128, win=10, new templates----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/150, train loss: 2.59169
Epoch 2/150, train loss: 0.65219
Epoch 3/150, train loss: 0.39640
Epoch 4/150, train loss: 0.30403
Epoch 5/150, train loss: 0.25871
Epoch 99/150, train loss: 0.12786
Epoch 100/150, train loss: 0.12716
Epoch 101/150, train loss: 0.13106
Epoch 102/150, train loss: 0.13327
Epoch 149/150, train loss: 0.12431
Epoch 150/150, train loss: 0.13312
Train Dataset Validation ==> TP: 0, FP: 0, TN: 79264, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 606, FP: 213, TN: 826, FN: 63
Test Dataset Validation  ==> Precision: 73.99%, Recall: 90.58%, F1: 81.45%

---2020/09/15-----epoch=150, hidden=128, win=15, new templates----------
===> Start training the execution path model ...
Slicing the multi-session logs with window 15 ...
Epoch 1/150, train loss: 2.23680
Epoch 2/150, train loss: 0.52090
Epoch 3/150, train loss: 0.32288
Epoch 99/150, train loss: 0.11875
Epoch 100/150, train loss: 0.11905
Epoch 101/150, train loss: 0.11270
Epoch 102/150, train loss: 0.10745
Epoch 147/150, train loss: 0.11583
Epoch 148/150, train loss: 0.12291
Epoch 149/150, train loss: 0.12040
Epoch 150/150, train loss: 0.11495
Train Dataset Validation ==> TP: 0, FP: 5, TN: 96890, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
Test Dataset Validation  ==> TP: 631, FP: 201, TN: 793, FN: 73
Test Dataset Validation  ==> Precision: 75.84%, Recall: 89.63%, F1: 82.16%

---2020/09/15-----epoch=150, hidden=128, win=10, new templates----------
===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Epoch 1/150, train loss: 2.19489
Epoch 2/150, train loss: 0.51872
Epoch 3/150, train loss: 0.32962
Epoch 98/150, train loss: 0.12633
Epoch 99/150, train loss: 0.13134
Epoch 100/150, train loss: 0.12714
Epoch 101/150, train loss: 0.12787
Epoch 148/150, train loss: 0.12789
Epoch 149/150, train loss: 0.12301
Epoch 150/150, train loss: 0.12789
Train Dataset Validation ==> TP: 0, FP: 2, TN: 98038, FN: 0
===> Start evaluating the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
Test Dataset Validation  ==> TP: 615, FP: 217, TN: 822, FN: 54
Test Dataset Validation  ==> Precision: 73.92%, Recall: 91.93%, F1: 81.95%


--------------v2.0.0, 10/17/2021-------------

Loglizer:

===> Train Model: DT

The number of anomaly logs is 11474, but it requires further processing
There are 9155 instances (sliding windows) in this dataset, cost 0:00:00.402512.

There are 406 log events
Among all instances, 3679 are anomalies
====== Transformed train data summary ======
Final train data shape: 9155-by-869

Normal training...: DT

Train validation:
Precision: 0.998, recall: 0.999, F1-measure: 0.999

===> Train Model: LR

The number of anomaly logs is 11474, but it requires further processing
Loading shuffled EventId list in templates: static version.
There are 9155 instances (sliding windows) in this dataset, cost 0:00:00.408505.

There are 406 log events
Among all instances, 3679 are anomalies
====== Transformed train data summary ======
Final train data shape: 9155-by-869

Normal training...: LR

Train validation:
Precision: 0.996, recall: 0.997, F1-measure: 0.996

===> Train Model: SVM

The number of anomaly logs is 11474, but it requires further processing
Loading shuffled EventId list in templates: static version.
There are 9155 instances (sliding windows) in this dataset, cost 0:00:00.406627.

There are 406 log events
Among all instances, 3679 are anomalies
====== Transformed train data summary ======
Final train data shape: 9155-by-869

Normal training...: SVM

Train validation:
Precision: 0.995, recall: 0.996, F1-measure: 0.996

===> Train Model: RFC

The number of anomaly logs is 11474, but it requires further processing
Loading shuffled EventId list in templates: static version.
There are 9155 instances (sliding windows) in this dataset, cost 0:00:00.445183.

There are 406 log events
Among all instances, 3679 are anomalies
====== Transformed train data summary ======
Final train data shape: 9155-by-869

Normal training...: RFC

Train validation:
Precision: 0.998, recall: 0.999, F1-measure: 0.999


===> Train Model: MNB

The number of anomaly logs is 6649, but it requires further processing
Building shuffled EventId list of templates.
There are 1217 instances (sliding windows) in this dataset, cost 0:00:00.041953.

There are 386 log events
Among all instances, 683 are anomalies
====== Transformed train data summary ======
Final train data shape: 1217-by-2000

First time training...: MNB

Train validation:
Precision: 0.918, recall: 0.922, F1-measure: 0.920

===> Train Model: PTN

The number of anomaly logs is 6649, but it requires further processing
Loading shuffled EventId list of templates.
There are 1217 instances (sliding windows) in this dataset, cost 0:00:00.041942.

There are 386 log events
Among all instances, 683 are anomalies
====== Transformed train data summary ======
Final train data shape: 1217-by-2000

First time training...: PTN

Train validation:
Precision: 0.777, recall: 0.941, F1-measure: 0.851

===> Train Model: SGDC_SVM

The number of anomaly logs is 6649, but it requires further processing
Loading shuffled EventId list of templates.
There are 1217 instances (sliding windows) in this dataset, cost 0:00:00.042135.

There are 386 log events
Among all instances, 683 are anomalies
====== Transformed train data summary ======
Final train data shape: 1217-by-2000

First time training...: SGDC_SVM

Train validation:
Precision: 0.826, recall: 0.908, F1-measure: 0.865

===> Train Model: SGDC_LR

The number of anomaly logs is 6649, but it requires further processing
Loading shuffled EventId list of templates.
There are 1217 instances (sliding windows) in this dataset, cost 0:00:00.042872.

There are 386 log events
Among all instances, 683 are anomalies
====== Transformed train data summary ======
Final train data shape: 1217-by-2000

First time training...: SGDC_LR

Train validation:
Precision: 0.890, recall: 0.857, F1-measure: 0.873


===> Train Model: MNB

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list of templates.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.001443.

There are 249 log events
Among all instances, 13 are anomalies
====== Transformed train data summary ======
Final train data shape: 27-by-2000

Incremental training...: MNB

Train validation:
Precision: 0.923, recall: 0.923, F1-measure: 0.923

===> Train Model: PTN

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list of templates.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.001470.

There are 249 log events
Among all instances, 13 are anomalies
====== Transformed train data summary ======
Final train data shape: 27-by-2000

Incremental training...: PTN

Train validation:
Precision: 1.000, recall: 0.923, F1-measure: 0.960

===> Train Model: SGDC_SVM

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list of templates.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.001556.

There are 249 log events
Among all instances, 13 are anomalies
====== Transformed train data summary ======
Final train data shape: 27-by-2000

Incremental training...: SGDC_SVM

Train validation:
Precision: 0.867, recall: 1.000, F1-measure: 0.929

===> Train Model: SGDC_LR

The number of anomaly logs is 516, but it requires further processing
Loading shuffled EventId list of templates.
There are 27 instances (sliding windows) in this dataset, cost 0:00:00.001499.

There are 249 log events
Among all instances, 13 are anomalies
====== Transformed train data summary ======
Final train data shape: 27-by-2000

Incremental training...: SGDC_LR

Train validation:
Precision: 1.000, recall: 0.923, F1-measure: 0.960


===> Train Model: MNB

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list of templates.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.274560.

There are 178 log events
Among all instances, 2658 are anomalies
====== Transformed train data summary ======
Final train data shape: 5664-by-2000

Incremental training...: MNB

Train validation:
Precision: 0.981, recall: 1.000, F1-measure: 0.990

===> Train Model: PTN

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list of templates.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.274985.

There are 178 log events
Among all instances, 2658 are anomalies
====== Transformed train data summary ======
Final train data shape: 5664-by-2000

Incremental training...: PTN

Train validation:
Precision: 0.998, recall: 0.999, F1-measure: 0.999

===> Train Model: SGDC_SVM

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list of templates.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.255109.

There are 178 log events
Among all instances, 2658 are anomalies
====== Transformed train data summary ======
Final train data shape: 5664-by-2000

Incremental training...: SGDC_SVM

Train validation:
Precision: 0.998, recall: 0.998, F1-measure: 0.998

===> Train Model: SGDC_LR

The number of anomaly logs is 2633, but it requires further processing
Loading shuffled EventId list of templates.
There are 5664 instances (sliding windows) in this dataset, cost 0:00:00.265383.

There are 178 log events
Among all instances, 2658 are anomalies
====== Transformed train data summary ======
Final train data shape: 5664-by-2000

Incremental training...: SGDC_LR

Train validation:
Precision: 0.998, recall: 1.000, F1-measure: 0.999


===> Train Model: MNB

The number of anomaly logs is 1676, but it requires further processing
Loading shuffled EventId list of templates.
There are 2244 instances (sliding windows) in this dataset, cost 0:00:00.119249.

There are 208 log events
Among all instances, 323 are anomalies
====== Transformed train data summary ======
Final train data shape: 2244-by-2000

Incremental training...: MNB

Train validation:
Precision: 0.705, recall: 1.000, F1-measure: 0.827

===> Train Model: PTN

The number of anomaly logs is 1676, but it requires further processing
Loading shuffled EventId list of templates.
There are 2244 instances (sliding windows) in this dataset, cost 0:00:00.114642.

There are 208 log events
Among all instances, 323 are anomalies
====== Transformed train data summary ======
Final train data shape: 2244-by-2000

Incremental training...: PTN

Train validation:
Precision: 1.000, recall: 1.000, F1-measure: 1.000

===> Train Model: SGDC_SVM

The number of anomaly logs is 1676, but it requires further processing
Loading shuffled EventId list of templates.
There are 2244 instances (sliding windows) in this dataset, cost 0:00:00.114327.

There are 208 log events
Among all instances, 323 are anomalies
====== Transformed train data summary ======
Final train data shape: 2244-by-2000

Incremental training...: SGDC_SVM

Train validation:
Precision: 0.994, recall: 1.000, F1-measure: 0.997

===> Train Model: SGDC_LR

The number of anomaly logs is 1676, but it requires further processing
Loading shuffled EventId list of templates.
There are 2244 instances (sliding windows) in this dataset, cost 0:00:00.114115.

There are 208 log events
Among all instances, 323 are anomalies
====== Transformed train data summary ======
Final train data shape: 2244-by-2000

Incremental training...: SGDC_LR

Train validation:
Precision: 1.000, recall: 1.000, F1-measure: 1.000

DeepLog:

===> Start training the execution path model ...
Building shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
100% 3732/3732 [01:53<00:00, 32.80Batches/s]
Epoch 1/150, train loss: 2.07581
100% 3732/3732 [01:52<00:00, 33.32Batches/s]
Epoch 2/150, train loss: 0.53035
100% 3732/3732 [01:52<00:00, 33.04Batches/s]
100% 3732/3732 [04:00<00:00, 15.51Batches/s]
Epoch 147/150, train loss: 0.16601
100% 3732/3732 [03:27<00:00, 17.99Batches/s]
Epoch 148/150, train loss: 0.25460
100% 3732/3732 [02:44<00:00, 22.73Batches/s]
Epoch 149/150, train loss: 0.20911
100% 3732/3732 [03:13<00:00, 19.27Batches/s]
Epoch 150/150, train loss: 0.24225
100% 3732/3732 [00:59<00:00, 62.48Batches/s]
Train Dataset Validation ==> TP: 0, FP: 76, TN: 119327, FN: 0

===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
100% 3774/3774 [01:48<00:00, 34.78Batches/s]
Epoch 1/150, train loss: 2.15389
100% 3774/3774 [01:41<00:00, 37.30Batches/s]
Epoch 2/150, train loss: 0.57234
100% 3774/3774 [01:43<00:00, 36.48Batches/s]
100% 3774/3774 [01:39<00:00, 38.12Batches/s]
Epoch 147/150, train loss: 0.16597
100% 3774/3774 [01:39<00:00, 37.85Batches/s]
Epoch 148/150, train loss: 0.16017
100% 3774/3774 [01:39<00:00, 37.75Batches/s]
Epoch 149/150, train loss: 0.15923
100% 3774/3774 [01:40<00:00, 37.44Batches/s]
Epoch 150/150, train loss: 0.16598
100% 3774/3774 [00:44<00:00, 84.03Batches/s]
Train Dataset Validation ==> TP: 0, FP: 32, TN: 120711, FN: 0


--------------v2.0.0, 10/17/2021-------------

commit e131d4a5
---------------
DeepLog:

===> Start training the execution path model ...
Building shuffled EventId list of templates.
Slicing the multi-session logs with window 15 ...
100% 3773/3773 [02:02<00:00, 30.84Batches/s]
Epoch 1/150, train loss: 2.02911
100% 3773/3773 [02:01<00:00, 30.99Batches/s]
Epoch 2/150, train loss: 0.48611
100% 3773/3773 [02:04<00:00, 30.36Batches/s]
Epoch 3/150, train loss: 0.32713
...
Epoch 100/150, train loss: 0.12688
100% 3773/3773 [02:04<00:00, 30.26Batches/s]
Epoch 101/150, train loss: 0.12225
100% 3773/3773 [02:06<00:00, 29.81Batches/s]
...
100% 3773/3773 [02:02<00:00, 30.73Batches/s]
Epoch 150/150, train loss: 0.14006
100% 3773/3773 [00:49<00:00, 76.89Batches/s]
Train Dataset Validation ==> TP: 0, FP: 33, TN: 120682, FN: 0

===> Start training the execution path model ...
Loading shuffled EventId list of templates.
Slicing the multi-session logs with window 10 ...
100% 3815/3815 [01:35<00:00, 39.88Batches/s]
Epoch 1/150, train loss: 2.21747
100% 3815/3815 [01:35<00:00, 39.85Batches/s]
Epoch 2/150, train loss: 0.59904
100% 3815/3815 [01:36<00:00, 39.45Batches/s]
Epoch 3/150, train loss: 0.40019
...
100% 3815/3815 [01:37<00:00, 39.19Batches/s]
Epoch 100/150, train loss: 0.13910
100% 3815/3815 [01:38<00:00, 38.75Batches/s]
Epoch 101/150, train loss: 0.14365
...
100% 3815/3815 [01:39<00:00, 38.38Batches/s]
Epoch 149/150, train loss: 0.14719
100% 3815/3815 [01:40<00:00, 38.08Batches/s]
Epoch 150/150, train loss: 0.15002
100% 3815/3815 [00:44<00:00, 86.22Batches/s]
Train Dataset Validation ==> TP: 0, FP: 7, TN: 122048, FN: 0
